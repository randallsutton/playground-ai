{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572ad5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "input_features = np.array([0.5, 0.1, -0.2])               # (3,)\n",
    "expected_output = 0.6                                     # scalar\n",
    "learning_rate = 0.5                                       # scalar\n",
    "weights_from_input_to_hidden = np.array([[0.5, -0.6],     # (3, 2) - 3 inputs to 2 hidden\n",
    "                                         [0.1, -0.2],\n",
    "                                         [0.1, 0.7]])\n",
    "weights_from_hidden_to_output = np.array([0.1, -0.3])     # (2,) - 2 hidden to 1 output\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "# Compute activations by passing weighted sums through sigmoid\n",
    "hidden_layer_activation = sigmoid(np.dot(input_features, weights_from_input_to_hidden))  # (2,)\n",
    "output_layer_activation = sigmoid(np.dot(hidden_layer_activation, weights_from_hidden_to_output))  # scalar\n",
    "\n",
    "### Backwards pass\n",
    "\n",
    "## Output layer\n",
    "\n",
    "# Error: difference between expected and actual output\n",
    "output_layer_error = expected_output - output_layer_activation  # scalar\n",
    "# Gradient: sigmoid derivative for the output neuron (how responsive is this neuron?)\n",
    "output_layer_gradient = output_layer_activation * (1 - output_layer_activation)  # scalar\n",
    "# Scaled error: how much should this neuron change? (how wrong × how responsive)\n",
    "output_layer_scaled_error = output_layer_error * output_layer_gradient  # scalar\n",
    "\n",
    "## Hidden layer\n",
    "\n",
    "# Error: blame assigned to each hidden neuron based on its connection strength\n",
    "hidden_layer_error = output_layer_scaled_error * weights_from_hidden_to_output  # (2,)\n",
    "# Gradient: sigmoid derivative for each hidden neuron (how responsive is this neuron?)\n",
    "hidden_layer_gradient = hidden_layer_activation * (1 - hidden_layer_activation)  # (2,)\n",
    "# Scaled error: how much should each hidden neuron change? (how wrong × how responsive)\n",
    "hidden_layer_scaled_error = hidden_layer_error * hidden_layer_gradient  # (2,)\n",
    "\n",
    "# Weight updates: how much should it change × how much did this input contribute x learning rate to reduce the delta\n",
    "delta_weights_from_hidden_to_output = learning_rate * output_layer_scaled_error * hidden_layer_activation  # (2,)\n",
    "delta_weights_from_input_to_hidden = learning_rate * np.outer(input_features, hidden_layer_scaled_error)  # (3, 2)\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_weights_from_hidden_to_output)\n",
    "\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_weights_from_input_to_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58555cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2513572524259881\n",
      "Train loss: 0.24996540718842905\n",
      "Train loss: 0.24862005218904504\n",
      "Train loss: 0.2473199321717981\n",
      "Train loss: 0.24606380465584854\n",
      "Train loss: 0.24485044179257037\n",
      "Train loss: 0.243678632018683\n",
      "Train loss: 0.24254718151769472\n",
      "Train loss: 0.24145491550165454\n",
      "Train loss: 0.24040067932493334\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from notebooks.backpropagation_data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Hyperparameters\n",
    "num_hidden_units = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learning_rate = 0.005\n",
    "num_records, num_features = features.shape\n",
    "previous_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights_from_input_to_hidden = np.random.normal(scale=1 / num_features ** .5,\n",
    "                                                size=(num_features, num_hidden_units))\n",
    "weights_from_hidden_to_output = np.random.normal(scale=1 / num_features ** .5,\n",
    "                                                 size=num_hidden_units)\n",
    "for e in range(epochs):\n",
    "    delta_weights_from_input_to_hidden = np.zeros(weights_from_input_to_hidden.shape)\n",
    "    delta_weights_from_hidden_to_output = np.zeros(weights_from_hidden_to_output.shape)\n",
    "    for x, y in zip(features.values.astype(float), targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_layer_input = np.dot(x, weights_from_input_to_hidden)\n",
    "        hidden_layer_activation = sigmoid(hidden_layer_input)\n",
    "        output_layer_activation = sigmoid(np.dot(hidden_layer_activation,\n",
    "                                                 weights_from_hidden_to_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        output_layer_error = y - output_layer_activation\n",
    "        \n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_layer_scaled_error = output_layer_error * output_layer_activation * (1 - output_layer_activation)\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_layer_error = np.dot(output_layer_scaled_error, weights_from_hidden_to_output)\n",
    "        \n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_layer_scaled_error = hidden_layer_error * hidden_layer_activation * (1 - hidden_layer_activation)\n",
    "        \n",
    "        # TODO: Update the change in weights\n",
    "        delta_weights_from_hidden_to_output += output_layer_scaled_error * hidden_layer_activation\n",
    "        delta_weights_from_input_to_hidden += hidden_layer_scaled_error * np.array(x[:, None], dtype=np.float64)\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_from_input_to_hidden += learning_rate * delta_weights_from_input_to_hidden / num_records\n",
    "    weights_from_hidden_to_output += learning_rate * delta_weights_from_hidden_to_output / num_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_layer_activation = sigmoid(np.dot(x, weights_from_input_to_hidden))\n",
    "        output_layer_activation = sigmoid(np.dot(hidden_layer_activation,\n",
    "                                                 weights_from_hidden_to_output))\n",
    "        mean_squared_error = np.mean((output_layer_activation - targets) ** 2)\n",
    "        if previous_loss and previous_loss < mean_squared_error:\n",
    "            print(\"Train loss:\", mean_squared_error, \"WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss:\", mean_squared_error)\n",
    "        previous_loss = mean_squared_error\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden_layer_activation = sigmoid(np.dot(features_test.values.astype(float), weights_from_input_to_hidden))\n",
    "output_layer_activation = sigmoid(np.dot(hidden_layer_activation, weights_from_hidden_to_output))\n",
    "predictions = output_layer_activation > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49319e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground-ai (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
