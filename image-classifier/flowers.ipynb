{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1600018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check torch version and CUDA status if GPU is enabled.\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available()) # Should return True when GPU is enabled. \n",
    "\n",
    "# Use GPU if it's available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd425fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"image-classifier/\"\n",
    "data_dir = root_dir + \"flowers\"\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a579b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your transforms for the training, validation, and testing sets\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \"valid\": transforms.Compose([\n",
    "        transforms.Resize(255),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize(255),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "}\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "image_datasets = {\n",
    "    \"train\": datasets.ImageFolder(train_dir, transform=data_transforms[\"train\"]),\n",
    "    \"valid\": datasets.ImageFolder(valid_dir, transform=data_transforms[\"valid\"]),\n",
    "    \"test\": datasets.ImageFolder(test_dir, transform=data_transforms[\"test\"]),\n",
    "}\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(image_datasets[\"train\"], batch_size=64, shuffle=True),\n",
    "    \"valid\": DataLoader(image_datasets[\"valid\"], batch_size=64, shuffle=False),\n",
    "    \"test\": DataLoader(image_datasets[\"test\"], batch_size=64, shuffle=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a653b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root_dir + 'cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(cat_to_name)\n",
    "\n",
    "model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.classifier)\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcfb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "epochs = 5\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 5\n",
    "\n",
    "print(f\"Total training batches: {len(dataloaders['train'])}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in dataloaders[\"train\"]:\n",
    "        print(f\"Running training at step {steps}...\")\n",
    "\n",
    "        batch_start = time.time()\n",
    "        steps += 1\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logps = model(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        print(f\"Device = {device}; Time per batch: {(time.time() - batch_start):.3f} seconds\")\n",
    "\n",
    "        if steps % print_every == 0:\n",
    "            print(f\"Running validation at step {steps}...\")\n",
    "\n",
    "            validation_start = time.time()\n",
    "\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in dataloaders[\"valid\"]:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    \n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            \n",
    "            print(f\"Device = {device}; Time per validation: {(time.time() - validation_start):.3f} seconds\") \n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Validation loss: {test_loss/len(dataloaders['valid']):.3f}.. \"\n",
    "                  f\"Validation accuracy: {accuracy/len(dataloaders['valid']):.3f}\")\n",
    "\n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "\n",
    "    print(f\"Device = {device}; Time per epoch: {(time.time() - epoch_start):.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "accuracy = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloaders[\"test\"]:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        ps = torch.exp(outputs)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "print(f\"Test accuracy: {accuracy/len(dataloaders['test']):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e220f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"class_to_idx\": image_datasets[\"train\"].class_to_idx,\n",
    "    \"classifier\": model.classifier,\n",
    "    \"epochs\": epochs\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, root_dir + \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_loaded = torch.load(root_dir + \"checkpoint.pth\", weights_only=False)\n",
    "\n",
    "model_loaded = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model_loaded.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_loaded.classifier = checkpoint_loaded[\"classifier\"]\n",
    "model_loaded.load_state_dict(checkpoint_loaded[\"state_dict\"])\n",
    "model_loaded.class_to_idx = checkpoint_loaded[\"class_to_idx\"]\n",
    "\n",
    "model_loaded.eval()\n",
    "\n",
    "model_loaded.to(device)\n",
    "\n",
    "print(f\"Trained for {checkpoint_loaded['epochs']} epochs\")\n",
    "print(f\"Number of classes: {len(checkpoint_loaded['class_to_idx'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "\n",
    "    # Resize so shortest side is 256, maintaining aspect ratio\n",
    "    width, height = image.size\n",
    "    ratio = 256 / min(width, height)\n",
    "    image = image.resize((int(width * ratio), int(height * ratio)))\n",
    "    \n",
    "    # Center crop to 224x224\n",
    "    crop_left = (image.width - 224) / 2\n",
    "    crop_top = (image.height - 224) / 2\n",
    "    image = image.crop((crop_left, crop_top, crop_left + 224, crop_top + 224))\n",
    "\n",
    "    # Convert to numpy array, scale to 0-1, and normalize\n",
    "    np_image = np.array(image) / 255.0\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    np_image = (np_image - mean) / std\n",
    "\n",
    "    # Set color channel to first dimension\n",
    "    return np_image.transpose((2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0706144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f21684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    with Image.open(image_path) as image:\n",
    "        np_image = process_image(image)\n",
    "        tensor_image = torch.from_numpy(np_image).float().unsqueeze(0)\n",
    "        tensor_image = tensor_image.to(next(model.parameters()).device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_image)\n",
    "\n",
    "        # Get probabilities\n",
    "        ps = torch.softmax(output, dim=1)\n",
    "\n",
    "        # Get top-k probabilities and indices\n",
    "        top_p, top_idx = ps.topk(topk, dim=1)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        top_p = top_p.cpu().numpy().squeeze()\n",
    "        top_idx = top_idx.cpu().numpy().squeeze()\n",
    "\n",
    "        # Invert class_to_idx to get idx_to_class\n",
    "        idx_to_class = {v: k for k, v in model.class_to_idx.items()}\n",
    "\n",
    "        # Map indices to class labels\n",
    "        top_classes = [idx_to_class[idx] for idx in top_idx]\n",
    "\n",
    "        return top_p, top_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction(image_path, model, cat_to_name, topk=5):\n",
    "    \"\"\"Display an image alongside a bar chart of the top-k predicted classes.\"\"\"\n",
    "\n",
    "    # Get predictions\n",
    "    probs, classes = predict(image_path, model, topk)\n",
    "\n",
    "    # Get flower names\n",
    "    flower_names = [cat_to_name[cls] for cls in classes]\n",
    "\n",
    "    # Create figure with two subplots\n",
    "    _, (ax1, ax2) = plt.subplots(figsize=(12, 5), ncols=2)\n",
    "\n",
    "    # Display image\n",
    "    with Image.open(image_path) as img:\n",
    "        ax1.imshow(img)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(flower_names[0])\n",
    "\n",
    "    # Display bar chart (horizontal, with highest probability at top)\n",
    "    y_pos = np.arange(len(flower_names))\n",
    "    ax2.barh(y_pos, probs)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(flower_names)\n",
    "    ax2.invert_yaxis()  # Highest probability at top\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fydwqea5f9i",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(test_dir + \"/1/image_06743.jpg\", model_loaded, cat_to_name)\n",
    "display_prediction(test_dir + \"/2/image_05100.jpg\", model_loaded, cat_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa33c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground-ai (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
